#!/usr/bin/env python3
"""Script to index PDF files using local embeddings."""

import sys
import os
from pathlib import Path

# Add src to Python path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

async def index_pdfs_local():
    """Index all PDF files using local embeddings."""
    try:
        print("ğŸ” PDF íŒŒì¼ ì¸ë±ì‹± ì‹œì‘ (ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©)...")
        
        # Import required modules
        from chancerag.core.document_processor import DocumentProcessor
        from chancerag.core.vector_store import FAISSVectorStore
        
        # Settings
        upload_path = Path("./data/uploads")
        vector_store_path = Path("./data/vector_store")
        chunk_size = 1000
        chunk_overlap = 100
        
        # Check if upload directory exists
        if not upload_path.exists():
            print(f"âŒ ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {upload_path}")
            return
        
        # Find PDF files
        pdf_files = list(upload_path.glob("*.pdf"))
        if not pdf_files:
            print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {upload_path}")
            return
        
        print(f"ğŸ“ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        for pdf_file in pdf_files:
            print(f"  - {pdf_file.name}")
        
        # Initialize document processor
        print("ğŸ“„ ë¬¸ì„œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì¤‘...")
        document_processor = DocumentProcessor(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
        
        # Initialize vector store with local embeddings
        print("ğŸ”¢ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘ (ë¡œì»¬ ì„ë² ë”©)...")
        vector_store = FAISSVectorStore(
            embedding_model="sentence-transformers/all-MiniLM-L6-v2",
            use_openai=False,  # Use local embeddings
            index_path=str(vector_store_path),
        )
        
        # Process each PDF file
        all_documents = []
        for pdf_file in pdf_files:
            print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
            
            try:
                # Load and process document
                documents = document_processor.load_pdf(str(pdf_file))
                print(f"  âœ“ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ì²­í¬")
                all_documents.extend(documents)
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        if all_documents:
            # Add all documents to vector store
            print(f"\nğŸ’¾ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€ ì¤‘... (ì´ {len(all_documents)}ê°œ ì²­í¬)")
            vector_store.add_documents(all_documents)
            
            # Save vector store
            print("ğŸ’¾ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì¤‘...")
            vector_store.save()
            print("âœ… ì¸ë±ì‹± ì™„ë£Œ!")
            
            # Show statistics
            stats = vector_store.get_stats()
            print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
            print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {stats.get('total_documents', 0)}")
            print(f"  - ì´ ì²­í¬ ìˆ˜: {stats.get('total_chunks', 0)}")
            print(f"  - ë²¡í„° ì°¨ì›: {stats.get('vector_dimension', 0)}")
        else:
            print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import asyncio
    asyncio.run(index_pdfs_local())

